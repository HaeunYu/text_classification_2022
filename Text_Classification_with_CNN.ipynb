{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SzYaOVGfyJkU"
   },
   "source": [
    "## 필요한 패키지 및 기본 제공 함수 (Requirements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u1kda-nkphwU"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/HaeunYu/text_classification_2022.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nw-d2AtyyaeM",
    "outputId": "0c09cb9b-abea-463a-aed1-9d1403e5d3fc"
   },
   "outputs": [],
   "source": [
    "!pip install PyKomoran\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oD13yV3DytFL",
    "outputId": "7b398b8a-bdbe-4627-cb47-75c3233cf4a5"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HM26uKr2wv85"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from tqdm import trange\n",
    "import numpy as np\n",
    "import random\n",
    "from nltk import sent_tokenize\n",
    "from PyKomoran import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hndzZAC4w7Ay"
   },
   "outputs": [],
   "source": [
    "def load_data(path) :\n",
    "  with open(path) as f :\n",
    "    data = json.load(f)\n",
    "\n",
    "  return data\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "# end of set_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xn79cCWoyQcs"
   },
   "source": [
    "## 데이터 로드 및 전처리 (Preprocess)\n",
    "\n",
    "\n",
    "\n",
    "1.   PyKomoran을 이용하여 train data와 test data를 형태소 분석 (품사 종류와 상관없이 모든 형태소를 이용함)\n",
    "2.   형태소 분석한 train data를 이용하여 word2idx 구축\n",
    "3.   train data의 label들을 이용하여 label2idx 구축\n",
    "4.   word2idx와 label2idx를 이용하여 train data와 test data의 word와 label들을 index로 변환\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ehOQfGj2xjos"
   },
   "outputs": [],
   "source": [
    "train_path = \"./text_classification_2022/newsdata_train.json\"\n",
    "test_path = \"./text_classification_2022/newsdata_test.json\"\n",
    "label_list = ['IT', '경제', '문화', '스포츠', '정치']\n",
    "\n",
    "train_data = load_data(train_path)\n",
    "test_data = load_data(test_path)\n",
    "\n",
    "komoran = Komoran(\"EXP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xwQ-NMTuw63f"
   },
   "outputs": [],
   "source": [
    "# 실습 2-1\n",
    "# word2idx, labels2idx, idx2labels 를 반환하는 함수를 완성해주세요.\n",
    "\n",
    "def create_w2i_l2i_i2l(data, label_list, komoran) :\n",
    "\n",
    "    # 이곳에 코드를 작성해주세요 #\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ------------------------- #\n",
    "\n",
    "    return word2idx, labels2idx, idx2labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HzxQZTztxof_"
   },
   "outputs": [],
   "source": [
    "word2idx, labels2idx, idx2labels = create_w2i_l2i_i2l(train_data, label_list, komoran)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6PBNjgNDxtJy"
   },
   "outputs": [],
   "source": [
    "def convert_examples_to_features(data, word2idx, labels2idx, komoran, max_length=512):\n",
    "    input_ids = list()\n",
    "    labels = list()\n",
    "\n",
    "    for doc in data:\n",
    "        doc_ids = []\n",
    "        sentences = sent_tokenize(doc[\"content\"])\n",
    "        for sentence in sentences:\n",
    "            doc_ids.extend(\n",
    "                [\n",
    "                    word2idx[w if w in word2idx else '<UNK>']\n",
    "                    for w in komoran.get_plain_text(sentence).split(' ')\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        if len(doc_ids) < max_length:\n",
    "            doc_ids += [word2idx['<PAD>']] * (max_length - len(doc_ids))\n",
    "            \n",
    "        elif len(doc_ids) > max_length:\n",
    "            doc_ids = doc_ids[:max_length]\n",
    "\n",
    "        input_ids.append(doc_ids)\n",
    "        labels.append(labels2idx[doc[\"topic\"]])\n",
    "\n",
    "    return input_ids, labels\n",
    "\n",
    "\n",
    "def make_dataset(input_ids, labels):\n",
    "    return torch.utils.data.TensorDataset(torch.tensor(input_ids, dtype=torch.long),\n",
    "                                          torch.tensor(labels, dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kQfEfkz-xuwe"
   },
   "outputs": [],
   "source": [
    "train_inputs, train_labels = convert_examples_to_features(train_data, word2idx, labels2idx, komoran, max_length=512)\n",
    "train_dataset = make_dataset(train_inputs, train_labels)\n",
    "\n",
    "test_inputs, test_labels = convert_examples_to_features(test_data, word2idx, labels2idx, komoran, max_length=512)\n",
    "test_dataset = make_dataset(test_inputs, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2AJMRoVWyEtt"
   },
   "source": [
    "## 모델 학습 (Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rPPNJHUGvke7"
   },
   "outputs": [],
   "source": [
    "# torch.permute 사용 예시\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1cbakatS0au9"
   },
   "outputs": [],
   "source": [
    "# 실습 2-2\n",
    "# torch.nn.Embedding 함수와 torch.nn.Conv1d 함수를 사용하여 모델을 구현해주세요\n",
    "\n",
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, output_dim):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # 이곳에 코드를 작성해주세요 #\n",
    "        self.word_embed = \n",
    "        self.conv_layer1 = \n",
    "        self.conv_layer2 = \n",
    "        self.conv_layer3 = \n",
    "        # ------------------------- #\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        self.fc = torch.nn.Linear(3 * 30, output_dim, bias=True)\n",
    "\n",
    "# 실습 2-3\n",
    "# 2-2에서 구현한 layer를 사용하여 모델의 forward 함수를 구현해주세요\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # 이곳에 코드를 작성해주세요 #\n",
    "        embedded = \n",
    "\n",
    "        conv1 = \n",
    "        conv1 = torch.nn.functional.relu(conv1.max(1)[0]) # max pooling\n",
    "\n",
    "        conv2 = \n",
    "        conv2 = torch.nn.functional.relu(conv2.max(1)[0]) # max pooling\n",
    "\n",
    "        conv3 =\n",
    "        conv3 = torch.nn.functional.relu(conv3.max(1)[0]) # max pooling\n",
    "\n",
    "        # ------------------------- #\n",
    "\n",
    "        x = torch.cat([conv1, conv2, conv3], dim=1)\n",
    "\n",
    "        output = self.fc(self.dropout(x))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PdqtzLw3xRD6"
   },
   "outputs": [],
   "source": [
    "def train(model, train_dataset, args):\n",
    "\n",
    "    set_seed(42)\n",
    "\n",
    "    train_batch_size = args[\"train_batch_size\"]\n",
    "    num_train_epochs = args[\"num_train_epochs\"]\n",
    "    device = args[\"device\"]\n",
    "    learning_rate = args[\"learning_rate\"]\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Loss function\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    train_dataLoader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=train_batch_size)\n",
    "\n",
    "    train_iterator = trange(num_train_epochs, desc=\"Epoch\")\n",
    "\n",
    "    print(\"\\n***** Running training *****\")\n",
    "    print(\"  Num examples = {}\".format(len(train_dataset)))\n",
    "    print(\"  Num Epochs = {}\".format(num_train_epochs))\n",
    "    print(\"  Train Batch size = {}\".format(train_batch_size))\n",
    "    print(\"  Device = \", device)\n",
    "\n",
    "    model.to(device)\n",
    "    model.train(True)\n",
    "    model.zero_grad()\n",
    "    for epoch in train_iterator:\n",
    "        loss = 0\n",
    "        for batch in train_dataLoader:\n",
    "            input_vector = batch[0].to(device)\n",
    "            label = batch[1].to(device)\n",
    "            predict = model(input_vector)\n",
    "\n",
    "            loss = criterion(predict, label)\n",
    "            loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(\"\\n********** Train Result **********\")\n",
    "            print(\"  Epoch / Total Epoch : {} / {}\".format(epoch + 1, num_train_epochs))\n",
    "            print(\"  Loss : {:.4f}\".format(loss))\n",
    "\n",
    "    model.train(False)\n",
    "# end of train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QGkShwNax0e3"
   },
   "outputs": [],
   "source": [
    "# 실습 2-4\n",
    "# model 선언을 위한 input_dim과 output_dim 을 설정해주세요\n",
    "\n",
    "vocab_size = # 이곳에 코드를 작성해주세요 #\n",
    "output_dim = # 이곳에 코드를 작성해주세요 #\n",
    "\n",
    "\n",
    "args = dict()\n",
    "args[\"train_batch_size\"] = 64\n",
    "args[\"device\"] = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "args[\"learning_rate\"] = 0.0005\n",
    "args[\"num_train_epochs\"] = 500\n",
    "\n",
    "model = CNN(vocab_size, output_dim)\n",
    "\n",
    "train(model, train_dataset, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sdsodb2uyAuj"
   },
   "source": [
    "## 모델 학습 후 평가 (Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-qISV9N-xTl1"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, test_dataset, args, news_num=900):\n",
    "    test_dataLoader = torch.utils.data.DataLoader(test_dataset, shuffle=False, batch_size=1)\n",
    "\n",
    "    device = args[\"device\"]\n",
    "\n",
    "    print(\"***** Running evaluation *****\")\n",
    "    print(\"  Num examples = {}\".format(len(test_dataset)))\n",
    "    print(\"  Test Batch size = 1\")\n",
    "\n",
    "    model.eval()\n",
    "    pred = None\n",
    "    label = None\n",
    "    for batch in test_dataLoader:\n",
    "        input_vector = batch[0].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predict = model(input_vector)\n",
    "\n",
    "        if pred is None:\n",
    "            pred = predict.detach().cpu().numpy()\n",
    "            label = batch[1].numpy()\n",
    "        else:\n",
    "            pred = np.append(pred, predict.detach().cpu().numpy(), axis=0)\n",
    "            label = np.append(label, batch[1].numpy(), axis=0)\n",
    "\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "\n",
    "    news_num -= 800\n",
    "    sample_pred = pred[news_num]\n",
    "    sample_label = label[news_num]\n",
    "    sample_result = {\"pred\": sample_pred, \"label\": sample_label}\n",
    "\n",
    "    accuracy = (pred == label).sum() / 200\n",
    "\n",
    "    return accuracy, sample_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pfiGn_aox2AO"
   },
   "outputs": [],
   "source": [
    "accuracy, sample_result = evaluate(model, test_dataset, args, news_num=900)\n",
    "\n",
    "print(\"\\n********** Total Test Result **********\")\n",
    "print(\"  Accuracy {}\".format(accuracy))\n",
    "print(\"  Sample pred : {}\".format(sample_result[\"pred\"]))\n",
    "print(\"  Sample label : {}\".format(sample_result[\"label\"]))\n",
    "\n",
    "\n",
    "with open(\"CNN_result.txt\", \"w\") as fw :\n",
    "  fw.write(\"********** Total Test Result **********\")\n",
    "  fw.write(\"\\n  Accuracy {}\".format(accuracy))\n",
    "  fw.write(\"\\n  Sample pred : {}\".format(sample_result[\"pred\"]))\n",
    "  fw.write(\"\\n  Sample label : {}\".format(sample_result[\"label\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urnfPOqOx5Wl"
   },
   "source": [
    "## 전체 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R89rOJBdwx0w"
   },
   "outputs": [],
   "source": [
    "# main\n",
    "\n",
    "train_path = \"./text_classification_2022/newsdata_train.json\"\n",
    "test_path = \"./text_classification_2022/newsdata_test.json\"\n",
    "label_list = ['IT', '경제', '문화', '스포츠', '정치']\n",
    "\n",
    "train_data = load_data(train_path)\n",
    "test_data = load_data(test_path)\n",
    "\n",
    "komoran = Komoran(\"EXP\")\n",
    "\n",
    "args = dict()\n",
    "args[\"train_batch_size\"] = 64\n",
    "args[\"device\"] = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "args[\"learning_rate\"] = 0.0005\n",
    "args[\"num_train_epochs\"] = 500\n",
    "\n",
    "word2idx, labels2idx, idx2labels = create_w2i_l2i_i2l(train_data, label_list, komoran)\n",
    "\n",
    "train_inputs, train_labels = convert_examples_to_features(train_data, word2idx, labels2idx, komoran, max_length=512)\n",
    "train_dataset = make_dataset(train_inputs, train_labels)\n",
    "\n",
    "test_inputs, test_labels = convert_examples_to_features(test_data, word2idx, labels2idx, komoran, max_length=512)\n",
    "test_dataset = make_dataset(test_inputs, test_labels)\n",
    "\n",
    "input_dim = len(word2idx)\n",
    "output_dim = len(label_list)\n",
    "\n",
    "model = CNN(input_dim, output_dim)\n",
    "\n",
    "train(model, train_dataset, args)\n",
    "\n",
    "accuracy, sample_result = evaluate(model, test_dataset, args, news_num=900)\n",
    "\n",
    "print(\"\\n********** Total Test Result **********\")\n",
    "print(\"  Accuracy {}\".format(accuracy))\n",
    "print(\"  Sample pred : {}\".format(sample_result[\"pred\"]))\n",
    "print(\"  Sample label : {}\".format(sample_result[\"label\"]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "(실습) Text Classification with CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
